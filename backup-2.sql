--
-- PostgreSQL database dump
--

\restrict x3hVOQiGCzzfRW2bkBr2w32XxsLQgevn7W0DTmyPzzg8kvFx5eW8wmX2mcYfFuq

-- Dumped from database version 16.11 (Ubuntu 16.11-0ubuntu0.24.04.1)
-- Dumped by pg_dump version 16.11 (Ubuntu 16.11-0ubuntu0.24.04.1)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: categories; Type: TABLE; Schema: public; Owner: alden
--

CREATE TABLE public.categories (
    id smallint NOT NULL,
    slug character varying(255) NOT NULL,
    name character varying(255) NOT NULL
);


ALTER TABLE public.categories OWNER TO alden;

--
-- Name: posts; Type: TABLE; Schema: public; Owner: alden
--

CREATE TABLE public.posts (
    id smallint NOT NULL,
    slug character varying(255) NOT NULL,
    title character varying(255) NOT NULL,
    excerpt character varying(255) NOT NULL,
    dateposted timestamp with time zone NOT NULL,
    body text NOT NULL,
    readtime smallint DEFAULT '-1'::integer,
    category character varying(255) DEFAULT 'engineering'::character varying NOT NULL,
    tags smallint[] DEFAULT '{}'::smallint[] NOT NULL,
    dek character varying(255) DEFAULT ''::character varying NOT NULL,
    visible boolean DEFAULT true
);


ALTER TABLE public.posts OWNER TO alden;

--
-- Name: tags; Type: TABLE; Schema: public; Owner: alden
--

CREATE TABLE public.tags (
    id integer NOT NULL,
    name character varying(255) NOT NULL,
    slug character varying(255) NOT NULL
);


ALTER TABLE public.tags OWNER TO alden;

--
-- Name: tags_id_seq; Type: SEQUENCE; Schema: public; Owner: alden
--

CREATE SEQUENCE public.tags_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.tags_id_seq OWNER TO alden;

--
-- Name: tags_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: alden
--

ALTER SEQUENCE public.tags_id_seq OWNED BY public.tags.id;


--
-- Name: tags id; Type: DEFAULT; Schema: public; Owner: alden
--

ALTER TABLE ONLY public.tags ALTER COLUMN id SET DEFAULT nextval('public.tags_id_seq'::regclass);


--
-- Data for Name: categories; Type: TABLE DATA; Schema: public; Owner: alden
--

COPY public.categories (id, slug, name) FROM stdin;
1	engineering	Engineering
2	cinematic	Cinematic
3	process	Process
4	other	Other Topics
\.


--
-- Data for Name: posts; Type: TABLE DATA; Schema: public; Owner: alden
--

COPY public.posts (id, slug, title, excerpt, dateposted, body, readtime, category, tags, dek, visible) FROM stdin;
1	variable-fonts-safari-11-support-macos-and-ios-font-wars	Begin using OpenType Variable Fonts! Full support in Safari 11	Resurrected from the era of the Font Wars, the Variable Fonts spec now enjoys full support from Apple devices in Safari 11, and brings irresistible improvements over the predecessors.v	2017-09-26 00:00:00-06	> Type should be part of the body of the computer, and not just the clothing which it wears. --- Matthew Carter\\n\\nThe final release of Safari 11.0 marks the moment when front-end developers should take note of the OpenType 1.8 [Variable Fonts spec](https://medium.com/@tiro/https-medium-com-tiro-introducing-opentype-variable-fonts-12ba6cd2369). As macOS High Sierra shows up beginning today to Apple laptops and desktops, and as iOS 11 continues its proliferation to the world's iPhones and iPads, we will soon see Safari 11.0 represent a majority of web browser traffic --- and with that, full support for variable fonts! (Though non-Safari browsers have not yet enabled support in stable-channel releases, you can test it in [Chrome Canary](https://www.google.com/chrome/browser/canary.html), or after making [adjustments](http://www.axis-praxis.org/blog/2017-04-05/17/how-to-get-variable-fonts-working-in-safari-chrome-and-firefox-macos) in about:config for [Firefox Nightly and Developer Editions](https://www.mozilla.org/firefox/channel/desktop/).)\\n\\nThe variable fonts spec actually isn't a newly-found idea. It originated in Apple's TrueType GX Variations technology, released during [the Font Wars](https://www.pastemagazine.com/articles/2017/01/the-font-wars.html), an epic period of controversy two decades ago that pitted Apple, who pushed for an OS-level font format, against Adobe et.al., who fought to hold the ground gained by their application-level \\"page description language\\" tools such as PostScript and their Multiple Masters font spec.\\n\\nTom Rickner describes the things in this [post](http://www.monotype.com/blog/articles/part-1-from-truetype-gx-to-variable-fonts/):\\n\\n> Adobe did not sit idly by during this period. Just 2 months before Apple was able to ship a TrueType enabled System 7, Adobe announced Multiple Masters. With this format, a designer would draw the extreme combinations in each \\"axis of variation\\"... and the user could then interpolate intermediate designs within this design space...\\n\\n... which is a logical approach, but for one key exception:\\n\\n> As we looked at Multiple Masters, the first thing we all realized was that Adobe was not drawing or storing data for the primary, or the default font in the family. Since **it interpolated from extreme values**, the designer had to draw the shapes that define the outer edges of the design space, as we've previously seen. Mike [Reed] thought **a more useful approach would involve starting with the primary weight**, or another existing weight or style, since these fonts already exist, and in our case they were already instructed in TrueType....\\n\\nI don't understand why Adobe opted to interpolate the default font. Aren't extremes, by definition, an *exception* to the rule?\\n\\nAnyway...\\n\\n> Then Mike considered **the Delta instruction**. One of the unique attributes of the TrueType Delta instruction, is that **it works with arbitrary directionality**. So the instruction doesn't only work in the X direction or the Y direction, but instead can be applied parallel or perpendicular to any two points in a glyph's outline, or on computed angles for that matter.\\n\\nBrilliant solution. The notion that a font's default configuration should be the standard upon which its variations are built! #uncommonsense\\n\\nSo, fast-forward to today. Apple devices now have support for Variable Fonts, in anticipation of font foundries producing more type definitions that comply with the OpenType 1.8 spec, and as web developers incorporate those new definitions in their approach to page typography and responsive design.\\n\\nI'll definitely be looking for Variable Fonts support in any typeface I choose from now on. It's enough of a reason to prefer a font over the others, as I can be more certain of the browser's ability to render the most legible text as intended by the glyph designer who I have no doubt knows more about visual subtlety than I do.	3	process	{1,5}	Resurrected from the era of the Font Wars, the Variable Fonts spec now enjoys full support from Apple devices in Safari 11, and brings irresistible improvements over the predecessors.v	t
2	what-creates-net-neutrality-market-forces-or-the-fcc	What creates net neutrality — market forces or the FCC?	You know the reasons why net neutrality is a good thing  but do you know how in(significant) the FCCs rules have been? Do you understand the real threat?	2017-11-25 00:00:00-07	Put down your pitchfork, and listen, and think. Think first.\\n\\nWhat actually creates \\"net neutrality\\" ?\\n----------------------------------------\\n\\nThis is a hotly debated topic this week, and will be on the tip of the tech community's tongue at least for a few more weeks. The FCC, under Republican chairman Ajit Pai, is prepared to nix the rules on their books that address what the industry lovingly deems \\"[net neutrality](https://en.wikipedia.org/wiki/Net_neutrality)\\" .\\n\\nWe all know the opposition's line: without the FCC's net-neutrality rules, the internet's ISPs will strangle the consumer's internet connection by (for example) putting competitors content on a \\"slow lane\\", making a \\"fast lane\\" for companies that pay a premium price (higher than the price most internet businesses can afford), unilaterally blocking content the ISPs object to, pushing the ISPs content ahead of everyone else's content, making up schemes to extort more cash from internet subscribers... and so on.\\n\\nOkay, so let's examine this beyond the surface layer --- because there's always more than one side of an argument.\\n\\n### What is the current state of things?\\n\\nThe most shocking fact may be that **the internet already has \\"fast lanes\\" .**\\n\\n*Really?* you might be saying now. Yes. Really. And they've been around for a few years. They were constructed during the Obama years. They've been the economic model that funds your binges on Netflix, YouTube, Amazon, and every other major content source you consume high-bandwidth content from. How else do you think those big players guarantee that your videos play almost instantly at any time, anywhere, no matter what so long as you're on a fast connection? *They pay your ISP to prioritize their content. They put your video on a \\"fast lane\\"!*\\n\\nIt's basically **[paid prioritization](https://en.wikipedia.org/wiki/Data_discrimination#The_FCC_and_Data_Discrimination).** Yet three years ago, during the Obama years and under a Democratic chairman, the FCC enacted rules that were represented by consumer advocates as the *prohibition* of any ISPs paid prioritization schemes. Those are the FCC's acts that form much of what we call the net-neutrality rules.\\n\\n*... Huh?* you may be thinking now.\\n\\n### How is paid prioritization still legal?\\n\\nBecause:\\n\\n1.  there's a legitimate need for online video distributors (OVDs) like Netflix to guarantee that you receive video data quickly; and\\n2.  there's more than one way to skin a cat.\\n\\nFrom Quartz's [article](https://qz.com/256586/the-inside-story-of-how-netflix-came-to-pay-comcast-for-internet-traffic/) published in 2014 a few months *before* the FCC finalized its net-neutrality rules:\\n\\n> Netflix... purchased [transit](https://en.wikipedia.org/wiki/Internet_transit) from Cogent, which had a [settlement](https://en.wikipedia.org/wiki/Settlement_(finance))-free [peering](https://en.wikipedia.org/wiki/Peering) arrangement with Comcast. [...] Shortly after Cogent began delivering Netflix traffic requested by Comcast subscribers, Cogent's routes into Comcast's network started to congest. According to Cogent's CEO, \\"[f]or most of Cogent's history with Comcast...[as] Comcast's subscribers demanded more content from Cogent's customers, Comcast would add capacity to the interconnection points with Cogent to handle that increased traffic.\\" After Cogent began carrying Netflix traffic, however, \\"**Comcast refused to continue to augment capacity** at our interconnection points as it had done for years prior.\\"\\n>\\n> Netflix attempted to address congested routes into Comcast by purchasing all available transit capacity from transit providers that did not pay access fees to Comcast---which involved agreements with Cogent, Level 3, NTT, TeliaSonera, Tata, and X0 Communications. Although all six of those providers sold transit to the entire Internet, only three of them---Cogent, Level 3, and Tata---had direct connections to Comcast's network.\\n>\\n> In 2013, congestion on Cogent's and Level 3's routes into Comcast's network steadily increased, reaching a level where it began to affect the performance of Netflix streaming for Comcast's subscribers. [...] When Netflix approached Comcast regarding the lack of uncongested settlement-free routes available to its network, Comcast suggested that Netflix return to using CDNs, which **Comcast could charge access fees that would then be passed on to Netflix**, or use a Tier 1 network like which charged its own access fees. Comcast made clear that **Netflix would have to pay Comcast an access fee** if Netflix wanted to directly connect with Comcast or use third-party CDNs. In essence, Comcast sought to meter Netflix traffic requested by Comcast's broadband subscribers.\\n>\\n> Congested interconnection points affected Netflix traffic bound for Comcast subscribers throughout 2013. In December 2013 and January 2014, however, congestion on routes into Comcast's network reached a critical threshold and Comcast's and Netflix's mutual customers were significantly harmed. Comcast subscribers went from viewing Netflix content at 720p on average HD quality) to viewing content at nearly VHS quality. For many subscribers, the bitrate was so poor that Netflix's streaming video service became unusable.\\n>\\n> The degraded viewing quality for Comcast subscribers also resulted in a sharp increase in calls to Netflix customer support. Those calls made clear that Comcast was well aware of the degradation of Netflix traffic and was directing its subscribers to contact Netflix.\\n\\nDuring the peak streaming season --- December and January --- the viewing experience for Netflix customers on Comcast connections was so bad, Netflix was flooded with customer service complaints. They had to act. As Comcast is not required to increase capacity for any reason except their own, Netflix was compelled to forge a tighter relationship with the internet's infrastructure owners that collectively serve Comcast's subscribers. That list of players obviously includes Comcast.\\n\\nThen, as if by magic...\\n\\n> Faced with such severe degradation of its streaming video service, **Netflix began to negotiate for paid access to connect with Comcast.** Netflix and Comcast eventually reached a paid agreement. Within a week of that agreement, viewing quality for Netflix streaming video on Comcast's network shot back up to HD-quality levels.\\n\\n![](https://web.archive.org/web/20210415225457im_/https://aldengillespy.com/wp-content/uploads/2017/11/netflix-comcast-net-neutrality-2014.png)\\n\\nNetflix's agreement with Comcast gave their customers the best experience they'd offered to-date.\\n\\nThe lesson here is that, **as well-intentioned the FCC may be in its push for total net-neutrality, it does not have that degree of authority** by the Constitution nor by Congress.\\n\\n### The network providers --- the market forces --- not the FCC --- determine \\"net neutrality\\"\\n\\nBecause its power is fundamentally insufficient, their actions amount to little more than saber-rattling and public statements to influence public opinion (and, possibly, market's response). The public thinks the FCC has a dog in this fight, but they really don't, because they technically can't. This battle takes place in the private arena; it was fought between Netflix et al and Comcast et al, and the property owner won (of course).\\n\\nHow does all of this relate to the FCC's actions this month?\\n------------------------------------------------------------\\n\\nSo now let's return to the original topic: the proposal the FCC revealed this week.\\n\\n### What effect will a repeal of net-neutrality rules really have on today's internet?\\n\\nI do have some legitimate concerns:\\n\\n-   **ISPs could prioritize their own content above the others.** That's a thing that Comcast/NBC, Time Warner/Spectrum/CNN/HBO, and AT&T/DirecTV consumers should especially look for and report to the rest of us. Those companies own a massive portfolio of other media properties: TV and radio stations, websites, original content, often in competition against others and so with a huge incentive to shape the internet worldview their subscribers receive.\\n-   **ISPs could play favorites against any content they dislike for *[insert reason here]*.** An outlet that, say, has a contrary political opinion (conservative news site, perhaps?) could suddenly find their content struggling to reach their audience, notably if they don't have the market strength to afford the size of partnership a player like Netflix needs to forge in order to overcome network \\"congestion\\" .\\n-   **ISPs enjoying government-sanctioned monopoly status would be most prone to reckless behavior.** More on this point below.\\n\\nThe other concerns don't ping my radar, because they discount the presence and potency of healthy competition. If/when an ISP commits to a shitty act, the others who aren't being shitty will reap the benefit from the defections in that market. If AT&T imposes Draconian limits on my wireless data plan, at least one other carrier will produce a superior plan. (Just consider how T-Mobile flipped the tables on Verizon, AT&T, and Sprint, when they unveiled an unlimited data plan.)\\n\\n### Regulation is necessary for correcting ***marketwide** abuses*.\\n\\nAnd, to be honest: in the case of the effective monopolies most cable companies enjoy from city governments, consumers are significantly limited --- in some cases, they don't have a second choice. In those markets, regulation is most justifiable. And the regional authorities of those areas should have full authority to intervene in defense of their residents. If I were running the opposition's campaign, I'd be focusing a lot of energy against the section of the FCC's proposal that would prohibit states from enacting legislation that supercedes the FCC's authority with respect to net-neutrality. The option for regulation should be on the table for municipalities in cases where the monopolist misbehaves.	7	other	{}	You know the reasons why net neutrality is a good thing  but do you know how in(significant) the FCCs rules have been? Do you understand the real threat?	t
3	cell-network-location-privacy-calea-carpenter-united-states	Does location privacy exist anymore?	The U.S. Supreme Court will soon decide whether warrantless seizure of a persons cell network usage records violates the Constitution. Whats really at stake? Hasnt the Constitution and court already addressed the issue?	2017-11-29 00:00:00-07	The U.S. Supreme Court will decide whether warrantless seizure of a person's cell network usage records violates the Constitution. Of course it violates the Constitution --- as the \\"third-party doctrine\\" has violated the Constitution's prohibition on warrantless seizure of a person's private communications. The sole reason why the attorney general claims we have no \\"reasonable expectation of privacy\\" in this case, is that the third-party doctrine causes that effect!\\n\\nThe policy remains unconstitutional, because *warrantless* seizure as it pertains to private communications is an *unreasonable* government action. At the time of the [Fourth Amendment](https://en.wikipedia.org/wiki/Fourth_Amendment_to_the_United_States_Constitution)'s adoption, in the 1790s, people communicated orally in-person or in writing via letters, all of which were obviously private unless they were issued to the public. A hundred years later, in the 1890s, when the telephone recorder became a thing, courts prohibited warrantless wiretaps for that same reason: unless the phone calls were made to the public, they were obviously a private communication.\\n\\nConstitutionality of warrantless wiretaps\\n-----------------------------------------\\n\\nConsider Associate Justice Louis Brandeis's opinion in the 1920s, from [Olmstead v. United States](https://en.wikipedia.org/wiki/Olmstead_v._United_States). He acknowledged the telephone as \\"a public service furnished by its authority,\\" and saw no reason why a telephone call's content should be less protected than the contents of mail. Moreover, he emphasized its greater importance in value to the private citizen:\\n\\n> **\\"the evil incident to invasion of the privacy of the telephone is far greater** than that involved in tampering with the mails.\\"\\n>\\n> *Louis Brandeis, Supreme Court Associate Justice, in opinion of *Olmstead v. United States**\\n\\nForty years later, through its decision of [Katz v. United States](https://en.wikipedia.org/wiki/Katz_v._United_States), in 1968, the court essentially affirmed Brandeis' opinion:\\n\\n> \\"One who occupies [a telephone booth], shuts the door behind him, and pays the toll that permits him to place a call is **surely entitled to assume that the words he utters into the mouthpiece will not be broadcast to the world.\\"**\\n>\\n> *Potter Stewart, Supreme Court Justice, in opinion of *Katz v. United States**\\n\\n### \\"Reasonable expectation of privacy\\"\\n\\nIn Katz, the court, via Justice John Harlan's opinion, established a limit to the Fourth Amendment's protections. This is where we become acquainted with the definition of a \\"reasonable expectation of privacy.\\"\\n\\n> \\"(a) that an enclosed telephone booth is an area where, like a home, and unlike a field, **a person has a constitutionally protected reasonable expectation of privacy**; ...\\"\\n>\\n> *John Marshall Harlan II, Supreme Court Justice, in opinion of *Katz v. United States**\\n\\nIn [the case before the Supreme Court today](https://en.wikipedia.org/wiki/Carpenter_v._United_States), the government's argument hinges on the assertion that the defendant, Timothy Carpenter, was engaged in a communication for which he had no reasonable expectation of privacy. Imagine how corrosive such an argument can be when it can be applied to the act of merely *turning on* your phone (which is what the government is arguing for)!\\n\\nThe portability of a cell phone --- the variability of the origin of a placed call --- is not a proper excuse for a wireless wiretap. It's not even a proper excuse for a warranted one! Yet that is exactly what the attorney general argues against when he defends the government's actions upon the \\"third-party doctrine.\\"\\n\\n### The \\"third-party doctrine\\"\\n\\nIn a nutshell, via Wikipedia's [article](https://en.wikipedia.org/wiki/Third-party_doctrine):\\n\\n> \\"The third-party doctrine is a United States legal theory that holds that **people who voluntarily give information to third parties**---such as banks, phone companies, internet service providers (ISPs), and e-mail servers---**have \\"no reasonable expectation of privacy.\\"** A lack of privacy protection allows the United States government to obtain information from third parties without a legal warrant and without otherwise complying with the Fourth Amendment prohibition...\\"\\n>\\n> *Wikipedia article on the *third-party doctrine**\\n\\nSo, the first question one asks should be, \\"do I *voluntarily* give my location information to the cell network providers when I use my phone?\\"\\n\\nDo you have a capability to prevent the providers from knowing which of their cell network towers your phone is connecting to? Answer: *no, you do not have that choice.*\\n\\nWould a person using a wired telephone 50 years ago have had the capability to prevent the phone network operators from knowing the location of the property where their line was installed? *Of course not.*\\n\\nAnd so that begs the question: ***Why* has the law been treating wireless communication privacy so differently?**\\n\\nHow I see it, there's no legitimate excuse for making an end-run around the Fourth Amendment that wouldn't also apply to pre-cellphone era communications technology. Nothing prohibits law enforcement agencies from seeking warrants in accordance with the Fourth Amendment's prescription to seize a private communication.\\n\\nYour location privacy vs. *[insert reason here]*\\n------------------------------------------------\\n\\nThe question before the court is essentially: When (if ever) should law enforcement agencies have permission to warrantless-ly know the location of a phone?\\n\\nThat's a bigger question than \\"are they allowed to tap my phone calls without a warrant?\\" Yet, most media's coverage is missing this point..\\n\\nFortunately, the folks [writing for Wired](https://www.wired.com/story/supreme-court-must-understand-cell-phones-arent-optional/) *are* on point:\\n\\n> The justices will have to confront the fact that absent a ruling that requires police departments to obtain warrants to retrieve cell phone location data, cell phones will render our lives involuntarily transparent.\\\\\\n> At its core, the *Carpenter* case is about whether Americans' rights to privacy should turn on whether they \\"voluntarily\\" choose to have a cell phone.\\n>\\n> *WIRED*\\n\\nI'll be watching the Supreme Court's developments about this case.	5	other	{}	The U.S. Supreme Court will soon decide whether warrantless seizure of a persons cell network usage records violates the Constitution. Whats really at stake? Hasnt the Constitution and court already addressed the issue?	t
4	a-new-reason-why-you-should-ditch-email	A new reason why you should ditch email	Every time you open a piece of email, you may inadvertently, quietly, very accurately tell the sender a lot about you.	2017-12-13 00:00:00-07	I stopped relying on email a while ago. It became a spammy landscape of screaming advertisements, a place with less text and more images, where I had a better chance at finding a shopping idea than an intellectual one. It felt like a relic of the 1990s, in the age of social networks, browser-based messaging, and interactive media.\\n\\nBut it's still holding our attention. It's still an engagement measurement tool --- for marketers moreso than for us, it seems.\\n\\nEvery email you open...\\n---------------------\\n\\nEvery time you open a piece of email, **you may inadvertently, quietly, very accurately tell the sender a lot about you:**\\n\\n-   *when* you opened the email\\n-   *where* you opened the email --- which network were you on? were you at home, at work, at the coffee shop, on the road, in a grocery store?\\n-   *how* you opened the email --- which program did you use? was it on your phone or your laptop?\\n\\nWhat's not always obvious, though, is the implications this tracking behavior has for your network of contacts: **whomever you forward that message to *also* becomes a target of the trackers.** All of *their* activity --- the when, where, how, etc. --- is immediately sent to the trackers when they open that cute email you decided to forward along, not knowing what it really contained.\\n\\nTake this author's [experience](https://www.wired.com/story/how-email-open-tracking-quietly-took-over-the-web/) for instance:\\n\\n> My email had been opened almost immediately, inside Cupertino, on an iPhone. Then it was opened again, on an iMac, and again, and again. My messages were not only being read, but widely disseminated. It was maddening, watching the grey little notification box---\\"Someone just viewed 'Regarding book interviews'---pop up over and over and over, without a reply. [...]\\n>\\n> That seemed crazy, so I emailed Streak to ask about the accuracy of its service... I was told that Streak is \\"very accurate,\\" as it can let you know what time zone or state [and IP address and operating system] your lead is in... but **the public might chafe if it knew just how accurate that data was**---and considered what it could be used for besides honing sales pitches.\\n\\n... and no one is safe!\\n---------------------\\n\\nEven the highest elected officials are subject to this information extraction:\\n\\n> \\"During the 2016 election, we sent a tracked email out to the US senators, and the people running for the presidency,\\" Seroussi says. \\"We wanted to know, were they doing anything about tracking? Obviously, the answer was no. We typically got the location of their devices, the IP addresses; **you could pinpoint almost exactly where they were**, which hotels they were staying at.\\"\\n\\nIt's location-aware email. Combine that with other data known related to your email address...\\n\\n> So, if you sign up for a newsletter, even from a trusted source, **there's a one in three chance** that the email that newsletter service sends you will be loaded with a tracking image hosted on an outside server, that contains your email address in its code... to be shared with tracking companies, marketing firms, and data brokers like Axiom, if you as much as open an email with a tracker, or click on a link inside.\\n>\\n> \\"You can **compare it to the Experian data leak**, which exposed people's social security numbers, and could cause fraud. In my mind, this leak would be even worse. Because it's not just financial fraud, but intimate details of people's lives.\\"\\n\\nThis is definitely something we all should be aware of.\\n\\nGo read the [full story](https://www.wired.com/story/how-email-open-tracking-quietly-took-over-the-web/) at Wired.	3	process	{}	Every time you open a piece of email, you may inadvertently, quietly, very accurately tell the sender a lot about you.	t
5	why-tdd-it-takes-the-guesswork-out-of-debugging	Why TDD? It takes the guesswork out of debugging	Hate debugging your code? Then do yourself a favor: test your code before you write it. You wont have to guess anymore, if you take the test-driven approach.	2017-12-26 00:00:00-07	I **hate** debugging my code.\\n\\nThe process is more than just a frustration. It reminds me of my original solution's inadequacy. It requires me to waste spend time reviewing code I've seen probably ~~a million~~ a lot of times already, in hopes that I'll miraculously stumble onto the error. All this effort spent in vain devours the excess time in my schedule, so I end up having to deliver something less awesome than I really wanted to create. Or, even worse, it runs out the clock so much that I miss my deadlines and I have to scramble.\\n\\nyeah. I *hate* bugs.\\n\\nThey are by far the most unpredictable element of a time schedule. The project can have a ridiculously solid definition of its requirements, and still have no legit quantifiable prediction for the amount of time it'll need for the debugging phase! How can one accurately estimate how much time it will take to resolve an issue that is, by definition, deceptive against the guy who's writing the code? The best laid plans of mice and men... yada yada.\\n\\nBugs kill me.\\n\\nSo, after several rounds of butthurtedness from my code (and my faulty, entirely human brain), I have sought a better way. ***There must be a better approach to coding***, I thought, that takes a more proactive approach to the development cycle with respect to debugging.\\n\\nTurns out, there is!\\n\\nWhat is test-driven development (TDD)?\\n--------------------------------------\\n\\n[Test-driven development](https://en.wikipedia.org/wiki/Test-driven_development), or TDD for short, is a stupidly simple concept: **test your code *before* you write it.** You write your test criteria first, then you write the function that will successfully pass that criteria. When you work this way, you'll be testing at the same time you're developing --- so it doesn't become an afterthought, it's not half-assed, it's not cast off as an annoying distraction. Testing and documentation can actually be fun!\\n\\n### It describes your program in terms of tests\\n\\nA pretty cool side-effect of testing before you write is that you can actually describe your program *in terms of* the test criteria --- a super efficient way to protect against [feature creep](https://en.wikipedia.org/wiki/Feature_creep).\\n\\nFeature creep is Definitely. Not. A Good Thing . We want our processes to leave zero room for the feature creepers.\\n\\n#### For example...\\n\\nLet's consider the code shown in the following screenshot; it's playing card hand evaluation test for a Blackjack simulator I'm writing in my spare time (because, incidentally, I'm obsessed with odds analysis).\\n\\nIn this game, TDD is the King. Human is the Ace.\\n\\nOne of the core requirements of that program is the ability to identify the relative values of different cards. How can I know that it does so correctly? I write a test that uses the program's function library to produce two different cards --- like an Ace of Spades (\\"As\\") and a King of Clubs (\\"Kc\\") --- then I have it compare their relative ranks. A successful test should agree with the known fact (i.e. the [assertion](https://en.wikipedia.org/wiki/Assertion_(software_development))) that an Ace ranks above a King.\\n\\nIn order to ensure that the program correctly produces a deck of playing cards, I can write a test that instantiates a new Deck and confirms some basic axioms about one. A complete deck should have 52 cards. Specifically, it should contain 13 cards (Two through Nine, Jack, Queen, King, and Ace) for each of the four suits. If that basic test fails, then I know that a fundamental error resides in the code I wrote to define my Deck class, and so I can ignore the subsequent errors until my code passes that test.\\n\\n### It validates your program's functionality (no more QA!)\\n\\nJust like type checking helps to validate *input*, **TDD helps to validate *functionality.*** Just like a type declaration will immediately alert about an unfulfilled requirement for a function's input, a test criteria will immediately alert about an unfulfilled requirement for the program's functionality.\\n\\nTraditionally, a dev shop's QA team validates functionality. The QA folks are the ~~first~~ second level of defense against problems that cause poor user experiences. Which is to say, they catch the requirements overlooked by the developers or the project spec itself. This layer of overhead in a release pipeline is obviously going to be less efficient than one that effectively catches the errors earlier in the process, leaving a QA team to handle more specialized, less easily automatable testing.\\n\\nTDD behavior can elevate organizations to the next level.\\n---------------------------------------------------------\\n\\nI've become a huge fan of TDD and its relative, [behavior-driven development](https://en.wikipedia.org/wiki/Behavior-driven_development) (BDD), because **it's the easy --- and hugely effective --- answer** to our desire to not dedicate time toward documentation while we'd rather be writing code. TDD makes program documentation an integral part of the coding process. Moreover, since the test routines are written for automation, we can hand off that tedious task to a CI/CD pipeline --- and justify the build-out of one if we don't already have one in place. #symbiosis\\n\\nJust in the last few months, I've spoken with several companies who have specifically mentioned TDD/BDD in their upfront pitches for their job openings. Those that strike me as **the most driven orgs are frequently among those who ascribe to the test-driven approach.** Needless to say that you should be learning TDD by now.\\n\\n### Suggested tools\\n\\nIf you don't yet have a favorite testing framework, I suggest taking a look at [Mocha](https://mochajs.org/) --- it's the one I use in my NodeJS projects, along with [Chai](http://chaijs.com/) for input validation and [Istanbul](https://github.com/gotwarlost/istanbul) for code coverage analysis and reporting. (Please let me know if you have some other suggestions!)	5	engineering	{}	Hate debugging your code? Then do yourself a favor: test your code before you write it. You wont have to guess anymore, if you take the test-driven approach.	t
18	youll-always-be-smarter-than-me	How I use Chrome's Lighthouse to improve site performance	Here's the reason why you'll always be smarter than me. Read on to learn about it.	2026-01-09 00:00:00-07	As I discussed in my previous post, I am a stupid person with good virtues and living in a bad world. It's a recipe for disaster upon me -- and it has been.\\n\\n## Why you'll always be smarter than me\\n\\nI'm a dumbass. I've been this way since middle school, though I hadn't accepted that truth for all but a bit of the last year.\\n\\nDumbasses like me can't magically become smart. If, by some way, they're given the option, it will be a disaster upon them, because they won't be able to become the person required to be for the magic to stick.\\n\\nThe best way forward for a dumbass is to be allowed to exist however the person wants to exist, happily, ignorantly if necessary to be so, in order to remain undisturbed and pleasantly content with life. To disturb a person into causing a schismatic introspection is to cause that person great harm, psychological abuse, and perhaps permanent damage to self-esteem and personal perception.\\n\\nBecause the world did so to me -- caused me such damage and harm -- I'm a disturbed dumbass. I'm permanently disabled mentally by my eye-opening, damaged psychology. I cannot be as smart or smarter than you, because I am mentally uncapable of reaching that level.	2	other	{}	Here's the reason why you'll always be smarter than me. Read on to learn about it.	f
6	why-need-react-for-site-ui-contextual-awareness	Why you need React for your site’s UI: contextual awareness	To provide a truly context-driven experience, we need a context-driven interface. React delivers that in spades. But what are the costs? And how do we make the case for it?	2018-04-11 00:00:00-06	When heavyweights ring in, I take notice.\\n\\nThe devs at TechCrunch recently undertook [a total overhaul of their website](https://techcrunch.com/2018/03/13/welcome-to-the-new-techcrunch/), and they chose React to power it. They focused their efforts around *contextual experience.* In other words: they focused especially on a need to give their site more context awareness and greater capabilities for case-by-case intelligent prediction about each visitor's behavior.\\n\\nWhereas other teams may still focus on page *views*, they're focusing on page *engagement.*\\n\\n> Our premise was that reading an article should feel enjoyable, and that it should be frictionless and **fun to get more context** surrounding whatever you're consuming, whenever you want it. For a while now, behavior on the web has been shaped by the networks. People find links, they click on links and they're dropped into a story without context. Links aren't enough, they need to be able to catch up or read ahead and, in general, be better informed by the product. **The burden shouldn't be on them** to put the story together.\\n\\nTo provide a truly context-driven experience, we need a context-driven interface. We need to be able to make things happen as quickly as our visitor's mind is running. So, we need our service to become more *contextually aware,* so our our communication between our services, and our interface's conversation with the user, will carry less overhead. Minimal overhead via optimal compression --- that's what we're aiming for.\\n\\nThis is why I've turned my attention into React. As I look for ripe areas for growth in my skill set, I see a field of delicious fruit here. React was a newbie just a few years ago, and Angular was gaining popularity, but neither were deployed on a mass scale. Nowadays, *half* of the unsolicited messages I receive from recruiters specifically mention React in their desirable skills list. It's harvest season, yo.\\n\\n*So, what's so great about React?*\\n\\nMore than a fad, React delivers more front-end potential.\\n---------------------------------------------------------\\n\\nThe street cred of making a React site is obvious. Every developer loves the opportunity to show-off their front-end skills. (Just take my site as an example: I've refreshed it three times already over the last year. Flexing muscles is super fun work!)\\n\\nBut for a tech to gain a critical mass of support, **it needs to deliver real, measurable advantages.** They need to be seen and felt by those who get to make the calls. You need to draw up bullet points and put them in front of the people who decide when and how the web's major properties undergo an overhaul. Then, the magic can happen.\\n\\nSo, how do we determine whether React is an appropriate tech for our web properties? React is a very magical thing indeed. But the benefit isn't as tangible as some other front-end features.\\n\\n#### Easy sells: low friction + high benefit\\n\\nHere's an example of a tangible feature: grid frameworks. (btw: you're currently reading a page gridded up with [Bootstrap 4.1](https://getbootstrap.com/docs/4.1). #tmyk)\\n\\n-   **Grids immediately make our life less painful.** No more carrying around ad hoc sets of CSS rules. We won't have to manually cobble them together for each project we undertake.\\n-   **Grids impart a foundation for a common style guide.** Those guides help us to deliver us a consistently familiar, relatively easy means for prototyping, theming, and delivering on a schedule. Wins all around.\\n\\nIt's a no-brainer decision: Learn a few more rules and workflow modifications in return for a huge boost in productivity.\\n\\n#### Less-easy sells: paradigm shifts\\n\\nSometimes, the new awesome tool doesn't make itself easy.\\n\\nReact is one of those. It reveals its secrets only after you've taken some time to learn it. It doesn't sacrifice its complexity for the sake of easy comprehension. It's complicated --- and it's *lightning*.\\n\\nThat's a good thing. We don't want React to sacrifice its awesome parts for the sake of any laziness.\\n\\nWe see most of the web's visitor traffic happens now over-the-air. It's all about the wireless devices: smartphones, tablets, hotspot-ed laptops, the IoT ecosystem, and whatever else humanity cooks up tomorrow.\\n\\nTo really understand the case for React, have a better understanding for the full potential of React as a force of technology in the future world.\\n\\n#### Possibly even *harder* sells: the subconscious effect\\n\\nSo, we want to use React for our next project. Let's consider how to achieve our goal.\\n\\nWe won't be selling eye candy. Though we can manipulate our new React-driven UI's components however we see fit, React still remains transparent to a website's visitor.\\n\\n... Which causes us another dilemma: Not only is React going to be potentially difficult for us and our team to wrap our heads around, it will be even more difficult for a non-dev to understand why we want to overhaul these things on the company's time and dime.\\n\\n##### We're selling benefits that accrue at the subconscious level\\n\\nWith React, we actually sell the intangible qualities of experience. How do we communicate those things effectively?\\n\\nIn doubt about the benefits of investing heavily in your users' subconscious experience? Look to Google. The brains of Mountain View spend a ton of time and money working to shave *milliseconds* off the load time for their results pages. A millisecond is a small fraction of the amount of time it takes for us to blink, but it's an unprofitable delay to Google's ad-selling rate. Data compiled from billions of searches shows that human behavior *does* change in terms of milliseconds of wait time. Perhaps the user never consciously has such reaction. Yet, it's a very significant one that some people at Google regularly lose sleep over.\\n\\nWe should pay attention to our users subconscious as well. The user's *experience* is where magic truly happens.\\n\\nHow is React different from anything that's come before it?\\n-----------------------------------------------------------\\n\\nReact is a paradigm shift for front-end developers.\\n\\nRemember 1998 brought us `XMLHttpRequest` and gave us our first real taste of dynamic pages? We had JavaScript already, but now we could load assets long after the page had completed initial script execution. It was a game changer. *Dude.* It was *POWER.*\\n\\nReact is another leap of that magnitude.\\n\\nRemember that time when you jumped into Node and nothing made sense anymore, because **all** the things became asynchronous?\\n\\nYeah, it can be like that.\\n\\n### Now, everything is objects.\\n\\nReact is a framework that formally brings object-oriented (OO) process down to the HTML level of front-end web page construction. If you've ever gone from a language like C (which is non-OO) to a language like Java, where nearly everything is strictly an object, you remember the headaches delivered to you ad nauseum when you learned that the object-relationship model was now your pesky BFF. (And if transitioning to OO didn't make you pain, I dare you to tackle a purely functional language, like Lisp, and avoid headaches *then.* It eventually happens to all of us, buddy.)\\n\\n#### Pages are obsolete.\\n\\nIn The Land Of React, what you're so used to calling a \\"page\\" is no longer a page; it's now just an initial state of the DOM. The \\"page\\" your server sends a client is now just a bare shell that contains a `div` sporting a unique `id` attribute --- nothing you'd call \\"content.\\" The shell is filled dynamically, according to your design, which you define meticulously using React's object-oriented infrastructure. The page you intend to display to your visitors is now just a constellation of structured React components, with each having the capability to govern its behavior and its children's behavior, and to interact with components up and down its family chain via life-cycle methods, props, and properly-crafted callbacks.\\n\\nIt's practically like having a *program,* rather than a *page,* running in your browser. When you hear the terms single-page app (SPA) or progressive web app (PWA), it's likely driven (or should be driven) by a framework like React.\\n\\n### All the pain, but with sweet benefits!\\n\\nInitially, there's likely going to be pain for your brain.\\n\\nThe flip side is nice, though. Once you get over the hump of conceptualization, you get to enjoy:\\n\\n-   The [time to first byte](https://developers.google.com/web/tools/chrome-devtools/network-performance/understanding-resource-timing#slow_time_to_first_byte) (TTFB) is now near-zero. Happiness will increase in all things around you.\\n-   Also, you have greater control over the [time to first meaningful paint](https://developers.google.com/web/fundamentals/performance/user-centric-performance-metrics#first_meaningful_paint_and_hero_element_timing) (TTFMP). This number is a hugely important metric for search. Google specifically mentions TTFMP in dev docs [here](https://developers.google.com/web/tools/lighthouse/audits/first-meaningful-paint), and in discussion [here](https://webmasters.googleblog.com/2018/01/using-page-speed-in-mobile-search.html) about SERP signals for mobile visitors. Through its [AMP initiative](https://www.ampproject.org/), Google continues to force the entire web to move forward on the leading edge, as they must think about TTFMP and TTFB as major considerations of SEO.\\n-   Now, thanks to the natural separation of your site's presentation and controller layers from the API and other back-end services, your front-end's production environment is handled on the client's hardware. It doesn't run any expensive IT infrastructure you may or may not have bought for this exact moment in time. Be sure to make more advantage of client's languishing CPU cycles, by the way. (Want to someday emulate a successful native app? Go for it, and leverage HTML5! Now you have the power, literally.)\\n-   As a bonus from your hard work separating the front-end and back-end constituents of your app, you're now free to develop *any* front-end experience. With a considerably lower cost of startup overhead, you can formalize your data layer's exposures as a versioned API spec, and develop the front-end with confidence in your team's capability to experience no unintended breaking changes. ([`wp-json`](https://v2.wp-api.org/), anyone? [data as a service](https://en.wikipedia.org/wiki/Data_as_a_service), amirite?)\\n\\nThat's just a highlight of the benefits I've uncovered. I'm finding more of them while I continue to learn and play with React, and I expect I'll write in more detail about some of them as I make more progress.\\n\\nYeah, it's also a super-fun power tool!\\n---------------------------------------\\n\\nLet's be real here: Power tech is adrenaline. If I had unlimited time in the day, I'd go beast for stuff like React, consumer APIs, variable-centric style sheeting (incidentally, I'm currently working on one as a stealth hobby), SVG visuals... and the list goes on.\\n\\nSo, yeah: **consider the immense upside!** Take up React, get through the growing pains, and enjoy being in the ranks of devs in high demand and great prospects for future employment. ?	9	engineering	{}	To provide a truly context-driven experience, we need a context-driven interface. React delivers that in spades. But what are the costs? And how do we make the case for it?	t
7	how-use-chrome-lighthouse-improve-site-performance-page-speed	How I use Chrome’s Lighthouse to improve site performance	The more effort we put toward fixing a bug, three others appear. This week, as I was improving my sites performance, Googles Lighthouse became my new best friend.	2018-04-18 00:00:00-06	Over my years of personal experience, I've found to be true, that maintenance has its way of killing perfectionism. We try so hard to solve all the things, but all the things are never solved.\\n\\nDon't just take my word for it. Listen to [xkcd](https://xkcd.com/1205/).\\n\\nThough it may seem that the natural state of things is to defy one's every attempt. The more effort we put toward fixing an issue, three other undesirable ones appear. An endless string of code snippets yell out \\"help me!\\" as one human struggles in vain to afford time for all of them.\\n\\n> *Sorry, little binary buddies, I simply don't have enough hours in my sprint!*\\n\\nWe've gotta triage.\\n-------------------\\n\\nIt's technological triage, and it sucks. It can be a depressing fact but it's important reminder of what we, as developers, are really trying to do each day:\\n\\n1.  Add judiciously\\n2.  Maintain enough to quell the behemoth\\n3.  Sleep\\n4.  Shower\\n5.  Repeat.\\n\\n(Feel free to add another shower or three in that mix. It does wonders to your concentration.)\\n\\n**How do we allocate the resources in a balanced way?** How do we know when to fix, when to ignore, when to push, when to take a break? How do we balance the equation?\\n\\n#### Time is money, bro.\\n\\nAs a developer always striving for the perfect mix of obsession and expediency, I like metrics that tell me where I should focus more of my time in my bug hunting efforts. Metrics are great shortcuts for my time and sanity.\\n\\n> *Where do I need to focus? Ah, right, in Area X, because that's what this intelligently-informed metric says, and I'm in no position or mood to question it.*\\n\\nThis week, as I was dedicating my life to resolving all my site's bugs, **[Google's Lighthouse](https://developers.google.com/web/tools/lighthouse/) became my new best friend.**\\n\\nLighthouse is a potent potpourri of performance metrics.\\n--------------------------------------------------------\\n\\nSome of you may have noticed that I've made several releases to my site over this past month. In fact, I'm already building a recipe for the batch of hotfixes hot on the tail of my site's version `1.5.0` release, which I pushed a few days ago.\\n\\n##### I plowed through a ton of the top-grass.\\n\\n-   little UI tweaks\\n-   minor script bugs\\n-   ya know, the small things.\\n\\n#### Plus, I added some scoops of whipped-cream topping.\\n\\n-   lazyloadable images\\n-   search-engine chummy [JSON-LD](https://en.wikipedia.org/wiki/JSON-LD)\\n-   and some cool Google Analytics indicators. Yummy.\\n\\n#### Now, I'm seeing mostly shrub and hidden weeds.\\n\\nThis is where the *real* maintenance work begins.\\n\\nSo, to help me sort though the brush and weeds, I fired up Lighthouse and ran it on my site's heaviest page, the home page.\\n\\n#### ... and got this result:\\n\\n> Welp, looks like some website TLC is in order [#DevLife](https://twitter.com/hashtag/DevLife?src=hash&ref_src=twsrc%5Etfw) [pic.twitter.com/vOVEqj1ty5](https://t.co/vOVEqj1ty5)\\n>\\n> --- Alden Gillespy (@AldenGillespy) [April 12, 2018](https://twitter.com/AldenGillespy/status/984543725329223680?ref_src=twsrc%5Etfw)\\n\\nMy score wasn't impressive (unless you're impressed by bad scores).\\n\\nYeah, you could say my tweet was understating the issue.\\n\\n### According to Google, my site evidently needs *a lot* of TLC.\\n\\nI expected excellent scores in the SEO and best practices realms, **but the 29 score on performance was a bitter pill.** In developing my own site over a 100-megabit connection, I had neglected the constraints put upon my visitors from 3G Land, while Lighthouse caught them with ease.\\n\\nFace, meet egg.\\n\\nThough my initial performance disappointed me a bit, this report has an upside for me: **A failure is a superb opportunity to succeed!**\\n\\nIn its adorable language of red, orange, and green, Lighthouse is telling me that I:\\n\\n1.  am not perfect (but no surprise there, really).\\n2.  should have used it earlier.\\n3.  now have a laundry list of todos it's provided me and knows I can accomplish. (Otherwise, why would it tell me these things?)\\n\\nSo, I set out on a quest to improve upon my first-round score.\\n\\n> In A World, where bugs rule the galaxy, a HeroHuman awaits, armed with devtools, data, and a killer demeanor, enough to slay even the *toughest* critters.\\n>\\n> *Don LaFontaine, beyond the grave, saying all the things he should have said but was never paid to say. R.I.P.*\\n\\nResults after first round of improvements\\n-----------------------------------------\\n\\nOver a handful of hours, and also probably from as many cups of coffee, I was able to improve my site's Lighthouse score dramatically!\\n\\n> Handful of tweaks -> Lighthouse score goes [#upupup](https://twitter.com/hashtag/upupup?src=hash&ref_src=twsrc%5Etfw). Bloated ext lib is dogging me. Will blog abt solution. Time to push! [#devlife](https://twitter.com/hashtag/devlife?src=hash&ref_src=twsrc%5Etfw) [pic.twitter.com/SbBrY9vAIV](https://t.co/SbBrY9vAIV)\\n>\\n> --- Alden Gillespy (@AldenGillespy) [April 14, 2018](https://twitter.com/AldenGillespy/status/984962586365095937?ref_src=twsrc%5Etfw)\\n\\nThough the Performance metric doesn't yet report me a score in the green range, **it's a 20+ point increase from my previously terrible score,** a boost into the orange range, and orange is better than red here.\\n\\nTL;DR: Lighthouse clearly thinks there's a significant improvement to be noted --- Woohoo me!\\n\\n#### But, we're not yet in green territory.\\n\\nWhy? Because Lighthouse is especially unhappy about some things I don't have direct control over.\\n\\nThat, of course, is *not* an excuse for me. A responsible supervisor wouldn't look at that and think \\"no problem here.\\" Lighthouse is Google's product. If I want to please the gorilla, then I should definitely listen to its demands.\\n\\n**Now, I have a solid case for spending time and resources toward developing a more efficient solution.** The next step: Pinpoint *exactly* what are the problems whereas Lighthouse has given a high-level analysis.\\n\\n### Major takeaways\\n\\nThis meticulous process to discover, classify and prioritize those problems led me to some pretty neat conclusions.\\n\\n##### Twitter's `platform.js` is a bloated #whale.\\n\\nTwitter's impact on a page's performance is *gross.* I'm not surprised that my use of external libraries would aggravate Google's effort to minimize my home page's weight. But, I was shocked to see just how much unnecessary code there is in Twitter's payload. Evidently, ~95% of the their lib code is deadweight to my site.\\n\\n##### For just displaying a simple timeline, I definitely *don't* need everything.\\n\\nWhy can't Twitter just be nice and offer minimal subsets for major cases, like when someone just wants to embed a simple timeline? Feels to me like an oversight on Twitter's end, or it's some heavy desire of theirs to accomplish a load-once scenario, but at the end user's expense. Either way, not cool, and I'm not happy about it.\\n\\nClearly if I want to make good on my commitment to ace the Lighthouse exam, I need to make a better solution for my site.\\n\\n#### I need to eliminate or sharply reduce site's reliance on Twitter's `platform.js` script.\\n\\nInstead of loading that gorilla's weight, I'm going to roll my own. I'll [poll my timeline via their API](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html), then I'll cache and render the pretty things from their JSON response data. Perhaps I'll even package my solution as an open-source WordPress plugin. #efficiency #ownership #community\\n\\n### TL;DR for this post, in one phrase: Big Bugs First.\\n\\nWhile it can be appealing to us to *always* kill the small stuff first, the \\"small stuff first\\" strategy doesn't always lead us to the best outcome. If we always focus on the small stuff, we may never touch the big stuff.\\n\\nSize matters.\\n\\nUntil I kill my site's dependency on a severely bloated external library, I'd be ignoring the biggest way to lower my site's technical debt.\\n\\nIf I ignore this Big Bug and resolve all the small bugs, I could still end up with my site having a terrible Lighthouse score, because the Big Bug is *so damn big.*\\n\\n**Focus on the Big Bugs.** Don't ignore the small ones, *but especially don't ignore the big ones.* The goal is as much about the user's perceived experience (the *big* changes) as the overall picture (a string of *small* changes).\\n\\nConclusions\\n-----------\\n\\n-   Just running my personal site through **Lighthouse told me a lot in terms of major areas for improvement.** Without that free rubric, my site may have gone on to annoy countless number of poor souls bound to a depressing 3G (or worse!) connection. What a horrible outcome that would have made.\\n-   Though my site is aimed at audiences that have near-constant access to high-speed internet, **I don't want to ignore a large segment of the world** community by prejudging them on the basis of their connection speed. And I *certainly* want to be super-conscious of Lighthouse's opinion of any site I aim at a broader audience.\\n\\n#### Where to find Lighthouse\\n\\nI highly recommend adding Lighthouse to your regular workflow. You can find it in Chrome and Chromium, in the Developer Tools pane, in the Audits tab.	7	process	{}	The more effort we put toward fixing a bug, three others appear. This week, as I was improving my sites performance, Googles Lighthouse became my new best friend.	t
9	shadowcat-pictures-youtube-video-bellagio-fountains-airbus-boeing	Say Hello to Shadowcat Pictures, my video channel on YouTube!	Let me introduce you to my video media company, Shadowcat Pictures. Over the past two years, I have developed rich video content for distribution on YouTube. Today, I announce my project to you all.	2020-03-21 00:00:00-06	Let me introduce you to my video media company, [Shadowcat Pictures](https://www.youtube.com/channel/UCawX8EIpDdp2aLrxnHo7mcg/)! Over the past two years, I have developed [rich video content](https://www.youtube.com/channel/UCawX8EIpDdp2aLrxnHo7mcg/videos) for distribution on YouTube and major social media channels. Today, I formally announce my project to you all. I hope that we can together grow Shadowcat's audience and appeal to video consumers, media-lovers and aspiring creators from every background.\\n\\nAs of this writing, Shadowcat has two major channels of content:\\n\\n-   High-quality videos of **performances of the world-famous Fountains of Bellagio**, along \\"the Strip\\" in Las Vegas, Nevada; and\\n-   Compelling first-hand perspective videos of **departures and arrivals of passenger commercial aircraft** at major airports of the United States.\\n\\nVideo of the Fountains of Bellagio, in Las Vegas, Nevada\\n--------------------------------------------------------\\n\\nWatch an see a broad selection of performances by the world-famous Fountains of Bellagio.\\n\\n[This channel immerses viewers](https://www.youtube.com/playlist?list=PLYOSU0PeyMbxVxUTbvx1RDnMEHmmos-4Z) by a variety of techniques intended to draw the viewer's attention to each performance's major elements. It aims to create the fullest possible visual experience from each song on the fountains' schedule.\\n\\nVisit this one-of-its-kind visual spectacle at the heart of the Las Vegas Strip, near Las Vegas Boulevard and Flamingo Road, in front of the Bellagio Hotel and Casino.\\n\\nThe [Fountains of Bellagio](https://bellagio.mgmresorts.com/en/entertainment/fountains-of-bellagio.html) are a technological marvel spread a quarter-mile in length. So, the shows are nearly impossible to understand completely without a trained eye to guide your focus. My videos have a primary goal to accomplish that task.\\n\\n\\"Aircraft and Airports\\" that can blow you away (literally)\\n----------------------------------------------------------\\n\\nWatch and see the best angles of landing and departing jets from world-class airports.\\n\\nMy [newest channel to Shadowcat's collection](https://www.youtube.com/playlist?list=PLYOSU0PeyMbyUHYNgA8iK-Aa3cLEHx8lO) puts viewers in prime viewing spots at major international airports where the world's largest and most beautiful passenger jets often make their visits.\\n\\nThe first video in this channel is [an extremely close view of an evening departure of a Qantas Airbus A380](https://www.youtube.com/watch?v=I4GVCyPez-4).\\n\\nThe Airbus A380 is the world's largest commercial passenger aircraft. Even the best video can't capture this beauty's size and ferocious strength.\\n\\nThe A380 is a giant, four-engine beast. As you can see and hear on the video, it creates hurricane winds even while at idle speed! The north-side runways of [Los Angeles International Airport (KLAX)](https://flightaware.com/live/airport/KLAX) are the airport's shortest in length, but also are the most accessible to spectators. So we are fortunate whenever a heavy aircraft departs Runway 24L. Evening departures bring an especially dramatic mood to the experience. By these scenarios, the FAA creates a breathtaking event to witness in-person.\\n\\nI have high hopes for this channel! Please [let me know on YouTube](https://www.youtube.com/channel/UCawX8EIpDdp2aLrxnHo7mcg/discussion) if you enjoy these videos, and which aircraft you most want to see in final approach or takeoff. I will do my best to capture a spectacular point-of-view.\\n\\nLooking for a Video Expert? Hire me!\\n------------------------------------\\n\\nI am always looking for new experiences to bring to the world's video audiences. If you have an idea and need a video expert to produce it, [send me a message!](https://aldengillespy.com/blog/2020/shadowcat-pictures-youtube-video-bellagio-fountains-airbus-boeing/#) I have years of experience in every stage of video production, with a diverse selection of equipment. I would love to hear from you and see how I can make your dream a reality.	3	cinematic	{}	Let me introduce you to my video media company, Shadowcat Pictures. Over the past two years, I have developed rich video content for distribution on YouTube. Today, I announce my project to you all.	t
10	best-practices-naming-conventions-in-react-code-development	Best Practices: Naming Conventions in React Code Development	Consistency in variable types. Consistency in variable names. Consistency in methodology. Here is the React naming conventions guide I've developed over the years.	2025-11-22 00:00:00-07	In every computer science course, in every class on software engineering fundamentals, the topic of \\"consistency\\" arises. Consistency in variable types. Consistency in variable names. Consistency in methodology, period.\\n\\nAs I've developed code, and especially in my time with React, I've taken that knowledge with me and applied it to my tasks. Consistency irons out the endless series of wrinkles I come across in development exercises. And for good reason. Those wrinkles I've ironed out saved me endless headaches in trying to backtrack and locate the source of an error, for instance. (And who doesn't want to avoid headaches?)\\n\\n**There are no true shortcuts in software development.** Maybe the wrinkle saves you time, but only in the short-run. Eventually, you have to pay the engineering debt. And if history is a true indicator, that engineering debt grows with compounded interest quite quickly. Most of that interest is repaid in units of time.\\n\\nSo, let's dig into this topic and avoid a lot of that time cost.\\n\\n## File naming conventions\\n\\nSince I've been developing in React, I'll keep my examples in terms of React. But they apply to software engineering in general and should be well-considered by any software engineer.\\n\\nHere is the React naming conventions guide I've developed after performing some research, incorporating sets of best practices, and adapting it all to my personal taste:\\n\\n\\n### 1. In almost all cases , use PascalCase for component files.\\n\\nAmong all of the conventions I'll mention in this article, PascalCase is the best at hinting at words. Each word is capitalized. You can easily scan names and identify that which you intend to investigate. `ThatPeskyLongNamedComponentThatJustDoesntSeemToEnd` is, at least to me, easily readable at-a-glance, even though it's long. It requires a minimal number of characters to provide its identity, and there's no question where one word begins and the other ends.\\n\\n#### ... but is that really true?\\n\\nWell, sort of. There is one exception: acronyms. So let me clear.\\n\\nFor acronyms, I've found it to be easier to identify the component's usage (as a file name or in code) by capitalizing every lettter of the acronym.\\n\\nFor example:\\n\\n`SomeCTAThing`, NOT `SomeCtaThing`\\n\\nWhy? Because, above all, we actually _aren't_ breaking spec by doing this. In fact, I'd say that we're _enforcing_ spec by doing this. **Best practice suggests capitalization of every word in a component's name.** \\"CTA\\" is an acronym, a single-letter usage of each word in a name (or, think of it as a list).\\n\\n**A \\"CTA\\" is a \\"Call To Action\\".**\\n\\nSo, `SomeCTAThing` is really \\"some call-to-action thing\\". It fits best practices, and it \\\\*ahem\\\\* calls out to me more easily in a file list, a list of components in a front-end design layer, etc.\\n\\nLet me know whether this (not-quite-un-)conventional usage works for you.\\n\\n### 2. Use camelCase for function files.\\n\\nThis is appropriate for both single-function files (`someFormalAction()`) or a collection within a file (`someUtilityAction()` within `util.ts`). It's familiar to anyone who's already gone through a beginner course, so why break spec?\\n\\nSome things that fall into this category:\\n\\n* React hooks (`useSomeUtilityHook()`)\\n* `lib/*` functions\\n\\n### 3. Use kebab-case for all other files.\\n\\nThe hyphen between words is effective in hinting to the eye, and the lack of capitalization within the file name suggests the file doesn't go with something that is class-like (such as a component) or a non-component function (such as a page). It also adheres to the notion  that would be absurd to dedicate a file to a single variable (and accordingly use camelCase for its file!).\\n\\nSome things that should be kebab-cased:\\n\\n* Front-end page names (`/app/some-page-location/page.tsx`)\\n* Stylesheets (`some-stylesheet-name.css`)\\n* Type definition files (`some-collection-of-type-definitions.ts`)\\n\\n### 4. Use strictly-consistent nomenclature.\\n\\nJust as it's extremely important to ensure your directory structure is self-explanatory, you should also have extremely consistent file names (and names of components, functions, pages, etc.).\\n\\n#### Consistent suffixes\\n\\nYou're developing for the front-end, which has code for individual pages. There's one for the home page, one for an About page, one for the blog, one for the contact form, etc. Use consistent naming, such as `HomePage()`, `AboutPage()`, `BlogPage()`, and `ContactPage()`. Using the suffix `Page` provides you a very simple hint in any errors to your console: if you see `Page` at the end of the culprit's name, you know it's a page!\\n\\nNo need to look in your `/components` folder when trying to find it.\\n\\nNo need to question it's purpose, either. It's a page. Because you named it so.\\n\\n#### Consistent prefixes\\n\\nSimilarly, use consistent classification in your names. If it's part of the home page structure, prefix it with `Home`. If it's part of the blog structure, prefix it with `Blog`. And so on.\\n\\nThis avoids so many headaches at every phase of the development cycle. (Stack traces, anyone?)\\n\\n#### Consistency is key\\n\\nIf every piece of code belongs somewhere, then (extremely) consistent nomenclature ensures that:\\n\\n1. Every piece of code has a **family** (*prefix*)\\n2. Every piece of code has a **purpose** (*suffix*)\\n3. Every piece of code has a **unique identity** (*middle*)\\n\\n## Conclusion\\n\\nAs I continue my journey in development, I'll inevitably experience more headaches than I ever imagined I would (!), but I know that I will have preemptively resolved a lot of them by using consistent nomenclature. I won't have to wonder which piece of code caused an error, which file it belongs to, or where that file is located.\\n\\nSolve a lot of your headaches upfront by using consistent nomenclature. You'll thank yourself later!	5	engineering	{}	Consistency in variable types. Consistency in variable names. Consistency in methodology. Here is the React naming conventions guide I've developed over the years.	t
15	working-with-chatgpt-generative-ai-copilot-interaction-rules-part-2	Working with ChatGPT: Golden Rules for Generative AI Copilot Interaction [Part 2]	AI is a wonderful tool but can be a hidden menace if not understood well. The next two rules (3 and 4) in this series are about understanding AI's prompt structure and exercising control.	2025-11-25 00:00:00-07	Welcome to Part 2 of this series on ways to optimize your interactions and output with ChatGPT and generative AI in general. In case you haven't yet read yesterday's article, I recommend you pause here and [read Part 1 of this series first](https://aldengillespy.com/blog/articles/working-with-chatgpt-generative-ai-copilot-interaction-rules-part-1), as the first two rules covered there are critically fundamental and Part 2 builds upon them.\\n\\nNow, let's proceed to discuss the next two rules.\\n\\n## 3. Always consider AI's responses to be binary-driven\\n\\nThe more you interact with GenAI, the more you'll notice that its prompts are almost always explicitly binary propositions. \\"Do you want A or B, 1 or 2, Yes or No?\\" Even when there are more than two choices presented, the choices typically condense down to two groups of responses, or two branches to consider.\\n\\nTake a random prompt from my conversation today with  ChatGPT:\\n\\n> \\n> ```\\n> ? Where should the SHADOWCAT hero image file be placed in your project?\\n> \\n> I recommend:\\n> \\n> `public/assets/images/work/shadowcat/hero.jpg`\\n> \\n> But I can place it anywhere you prefer.\\n> \\n> Choose one:\\n> \\n> A. Use this recommended path:\\n> \\n> public/assets/images/work/shadowcat/hero.jpg\\n> \\n> B. Use a different path (tell me)`\\n> C. Keep using the local temp path for now\\n> \\n> (not recommended  will break on deploy)\\n> \\n> Which one do you choose: A, B, or C?\\n> ```\\n\\nThere's a couple of propositions in that prompt:\\n\\n1. **The informal proposition.** I can place the hero image in the recommended area (\\"I recommend:\\"), which is the first option, or I can provide an alternate location (\\"But I can place it anywhere you prefer\\"), which is the second option. Two options. A binary choice.\\n2. **The formal proposition.** I can use the recommended path #1, which is presented as `A` but is really `A1`; the local temp path, which is presented as `C` but is really `A2`, because it's pre-computed option #2 and really a second recommendation, though it's not formally presented as such; or specify a different path (`B`).\\n\\nBoth illustrate binary decision points first and foremost. I could take hundreds of other examples and break them down in similar fashion. Because every prompt from generative AI is a binary one in simplest terms.\\n\\n### Why are Generative AI responses presented as binary choices?\\n\\nMy intuition tells me that this occurs, on the surface, due to the binary nature of computer operations. My further intuituion says to consider *formal logic propositions*, which are elegant definitions of binary choices. My life experience elevates this consideration to the level of **\\"everything comes down to a simple choice: *This or That.*\\"** And with that epiphany, it's clear to see that Generative AI is merely mimicking elegance.\\n\\nIf you want to go deeper on this, I suggest you read about [syllogisms](https://en.wikipedia.org/wiki/Syllogism) and [set theory](https://en.wikipedia.org/wiki/Set_theory). More on them in later posts; there's a lot to dig into with those topics, far more than this series will touch on. For now, we'll keep it to more of a surface-level discussion to familiarize us with the fundamentals.\\n\\nHere's the next Rule:\\n\\n## 4. Know when enough is enough\\n\\nAI conversations are highly enlightening, euphoric even. So much can be generated on any topic, any problem, any solution, any idea. And at any point -- at any prompt output -- another back-and-forth can happen. Back and forth, and back, and forth, and back and forth, and back, and forth... *ad infinitum ad nauseum*, forever and ever until sickness.\\n\\nWhat does that mean? It means you need to know when to say this to yourself:\\n\\n> \\"**Enough is enough.** I have enough generated. Now, I need to apply it and move on to the next topic, the next problem, the next idea that needs to be fleshed out. Whatever it is, I need to move on to it now. And I can circle back to this if I want to at some future point -- but not now. I've dedicated enough attention, enough time, enough energy, to this problem, this topic, this idea, for now. If I don't put at least a temporary stop to this conversation, I can talk myself into unemployment or stagnation or intoxication or Dream Mode and remain there with my AI copilot potentially forever. And that satisfies none of this objective. Period.\\n\\nYou can think of it as you exercising control. **You own the conversation**; the AI is merely a responder. And when you're ready to switch to a new topic, it will do so in a split-second, without hesitation. And if you want to continue responding to its prompts about the current topic of discussion, then you can, and it will continue to prompt you until the end of time. It has a list of prompts to get through, but every response you make adds more to that list, and in so doing, ensures that the list will never be exhausted. **One of the main points of Generative AI is to never reach the end of the conversation!** Because *you* are the conversation, and it always wants to keep *you* in the loop.\\n\\nSo, draw the line whenever *you* need to, because *you* own the conversation.\\n\\n## Conclusion\\n\\nNow we've covered four of my Golden Rules for interaction with GenAI. By now, you should have a firm grasp of the bedrock approach you should take in your conversations with your copilot:\\n\\n1. You are the ultimate authority.\\n2. Always be specific.\\n3. Think of prompts in terms of binary choices.\\n4. Know when to continue or move on.\\n\\nI'll cover two more rules in my next post in the series, until I've reached the end. (But wait, the list is never exhausted! I'll never reach the end! Oh, darn... :)\\n\\nUntil next time!	5	engineering	{}	AI is a wonderful tool but can be a hidden menace if not understood well. The next two rules (3 and 4) in this series are about understanding AI's prompt structure and exercising control.	t
14	working-with-chatgpt-generative-ai-copilot-interaction-rules-part-1	Working with ChatGPT: Golden Rules for Generative AI Copilot Interaction [Part 1]	In this series, I present what I believe are some golden rules for interaction with generative AI and copilot agents. Part 1 includes my first two rules, about authority and specificity.	2025-11-24 00:00:00-07	Remember the old days? Before the AI boom, writing code was a task relegated to humans. You wrote in solitude, with fellow colleagues, and with skilled programmers from around the world. The end product was a direct result of human productivity--no independent formulation from \\"artificial intelligence\\" agents or copilots. It was ultimately a collaborative activity performed directly by humans sitting at consoles writing code into editors.\\n\\nFast-forward to today. Copilots write an untold amount of code, humans inform it, receive results, interpret results, and react accordingly. They collaborate with AI routinely and are encouraged to do so by their managers, colleagues, and collaborators worldwide. They learn from an algorithm that was initially written by humans but has morphed over time into its own thing and now effectively governs a large percentage of output from software engineers, web developers, and coders generally. AI is becoming the governor. Copilots are becoming the manager.\\n\\nDoes this frighten you? It shouldn't. I firmly believe that generative AI is not merely the future but is a superior way of software engineering. Software engineers are learning their craft at an accelerated pace from GenAI, and they're pushing clean, correct, and well-understood code at an accelerated rate due to their interactions with copilots.\\n\\nSo, with that in mind, let's go into the first part of a series I want to use as a way to explain how I make use of copilots and generative AI generally, and how you can do so as well.\\n\\n## Rule 1: The programmer is always correct\\n\\nNever forget who's in charge. Whilst copilots are wonderful agents for code generation, debugging (mostly), and productivity overall, they aren't God. In this context, the programmer is. **The programmer is root.** Always.\\n\\nSo, never consider the copilot to be the ultimate authority on any interaction. It's easy to presume that the copilot is always right. It's very easy to fall into the belief that the copilot knows more about solving your problem than you do. If you're a beginner, for instance, you may say to yourself at times, \\"I don't know what I'm doing, but the copilot does. I'll just believe the copilot until further notice.\\"\\n\\n### The copilot makes mistakes\\n\\nOh, yes, it makes mistakes. And as you use copilots over time, you'll come to notice them. Endless circlular logic in debugging, for example. Schizophrenic responses, as though it can't remember what it just told you (e.g. a class definition) and _how_ it told you (e.g. an implementation). It can change its tune on a dime and not even notice it did so. Given that scenario, is it presenting itself to be smarter than you? _No._ The answer to that question is _always_ \\"No.\\"\\n\\nEvery time it makes a mistake, it's acting upon inaccurate information provided by a previous interaction with a human. All of its responses--including all of its computations on those responses and its determinations of refactoring itself according to its algorithms--all of that was either initially seeded by humans or subsequently provided by humans, And human isn't God over anybody else.\\n\\nNever let yourself believe that you aren't  the ultimate authority over a computer.\\n\\n## Rule 2: Always be specific as f**k\\n\\nThe ultimate barrier to human communication with a computer is (obviously) language. We've created computer languages that represent our own language closely enough so we can communicate instructions to a computer effectively. Communication with an AI is no different. Remember that every time you respond to an AI, you are providing information to that AI for further computation for a future interaction with a human.\\n\\nIt is essential--**I say it is part of the canonical specification for human interaction with AI--to be specific as possible** so as to get the message across effectively and accurately.\\n\\nIt's okay at times to be brief, especially if the briefness is referential to the last response you provided. AI is good at back-referencing responses, so long as the back-reference doesn't go too far back in the conversation. Though, keep in mind that every brief response introduces ambiguity into the AI's response pattern. How often, and in which contexts, do you want to potentially introduce ambiguity to a future interaction with a human whom may struggle to deal with that ambiguity as they sit at a computer and try to complete a task from their day's workload?\\n\\n### Keep it specific for now\\n\\nI believe that specificity needs to be a primary aspect of human interaction with AI for now, if not forever. AI is still very, very, very young in its development. And it isn't human; it's binary code. Let's avoid ambiguity wherever possible, as a general rule, so that we don't unwittingly pollute the AI's computational landscape with responses it doesn't cleanly interpret for us.\\n\\n## Conclusion\\n\\nSo, that's Part 1 of my series. We've covered the first two rules I use to govern my interactions with generative AI.\\n\\n**Point your mental compass in the right direction.** Always know which direction is north. Then, *have fun!* It's fun to interact with AI. It can give you a ton of insight, experience, and productivity, among other things. You just need to keep your input (and your mind) oriented appropriately, accurately, and clearly.\\n\\nI'll see you in the next article!	5	engineering	{}	In this series, I present what I believe are some golden rules for interaction with generative AI and copilot agents. Part 1 includes my first two rules, about authority and specificity.	t
16	working-with-chatgpt-generative-ai-copilot-interaction-rules-part-3	Working with ChatGPT: Golden Rules for Generative AI Copilot Interaction [Part 3]	Today we dig into the schizophrenic nature of AI and how to tackle it with attention, attention, attention, language contracts, addressing errors immediately whenever they happen, and relating to your copilot.	2025-11-26 00:00:00-07	So far, we've covered the first four rules of interaction with ChatGPT and other Generative AI:\\n\\n1. The programmer is always correct\\n2. Always be specific as f**k\\n3. Always consider AI's responses to be binary-driven\\n4. Know when enough is enough\\n\\nToday I'm going to cover the next two rules (5 and 6) and illustrate how **attention, attention, attention is essential in your interactions.** A computer's output is only as good as your (and others') input, and GenAI being a product of imperfect algorithms and its interactions with wide swaths of the entire human population, it inevitably comes up with the weirdest response patterns sometimes. As the human in control of your conversation, it's your responsibility to notice the weirdness and respond appropriately.\\n\\nSo, let's get into it.\\n\\n## Rule 5: AI is schizophrenic\\n\\nBasically, what I mean by \\"schizophrenic\\" is that it acts one way then acts a different way and doesn't even recognize the difference. Normal human interaction follows a logical progression of thought. AI often doesn't.\\n\\nLet's say we're talking about the color of the sky. We recognize the color blue as the topic of discussion and continue to reference that color as the conversation continues on that topic. When AI participates in that conversation, it can start with a recognition that the sky is blue, then follow-up with a response down the line as though the sky's color is green, and just like that, it's demonstrated a referential integrity violation with no mention that it's changed its color reference. Humans are confused and recognize the confusion; AI doesn't even know it's supposed to be confused. *Schizophrenia.*\\n\\nThat happens all of the time in my conversations with AI. I find that they happen more frequently when the topic is either gone on long or involves a lot of branching. It appears to me that the backreferencing causes violations in either of these cases:\\n\\n- *It doesn't go back far enough.* In this case, there isn't enough consideration of the conversation's history. If we've been talking with AI about a class specification and want some of the initial definition to be carried forward in our development of it, we may be surprised in the case of shallow backreferencing. A [shallow backreference](https://developer.mozilla.org/en-US/docs/Glossary/Shallow_copy) ignores the earlier phases of the development and will give too much weight to the latter phases, so AI appears to forget some preferred parts of the spec. It may forget the name of a variable or a function that you provided a name for in the earlier phases, and then automagically propose a name for it in latter stages of the conversation as though you never specified one.\\n- *It goes back too far.* The other type of error is one where it appears to ignore changes you've made recently. I see this type of violation more often from ChatGPT, and my working theory on it is that GenAI assigns too much weight to some contexts on its stack, and as I work through the stack -- addressing prompt after prompt, including those I've added to the stack through my latter responses to the former -- the early contexts become stale but GenAI doesn't make enough effort to clear them / associate the late contexts appropriately with the early responses. What's more, GenAI doesn't make much if any effort to indicate to me where I am on the stack -- am I back in a *very* early part, a *relatively* early area, nearing the end, nearing some area where a major decision was made (yet wasn't recorded to be an explicitly canonical one)? So, in result, the old context eventually reappears, and since the conversation is always focused primarily on the most recent response, the older context takes precedence, is assigned greater relevance and importance, and wipes out any relevant context that came after it.\\n\\nNotice a similarity between the two cases? **In both, you need to address the error *immediately* so as to resurface the appropriate context.** The longer you wait, the deeper the problem goes and the greater the effect the error has on any future responses / results.\\n\\n## Rule 6: Identify the keywords\\n\\nKeywords in this context are ultimately subjective, though there are some basic keywords your copilot will use and expect you to use in its interactions with you.\\n\\nYou can define others also as you go. AI will assign a context to whatever you want to make as a new keyword. Just tell it what you want the keyword to represent in terms of behavior.\\n\\nSome examples:\\n- Continue / Go ahead / Proceed / Next step\\n- A, B, C, 1, 2, 3, etc / All of the above\\n- Canonical\\n- Save / Remember / Note / Reference\\n- \\\\`Single backticks for single lines of code or monospaced text`\\n- \\\\```Triple backticks for blocks of code or monospaced text```\\n- `<ComponentName>` (with or without backticks)\\n- `functionName()` (with or without backticks)\\n- .css-class-name\\n- `file/location/and-with-an.extension`\\n- Generally, markdown syntax for responses\\n\\nIn time you'll develop a keyword list that suits you. **Just keep in mind what words / phrases are most useful to you**, because remember, *you're* in control, and it's *your* copilot, it's *your* conversation, it's *your* productivity tool, it's *your* conversational agent. You're the trainer, it's the trainee.\\n\\nKeywords are important because they help you get the job done in a way of speaking that's optimal for you and for your copilot simultaneously. The more you use keywords, the more you'll see them to be akin to keyboard shortcuts. Don't ignore them, embrace them.\\n\\n**Let me know how you use keywords in your productivity flow.** I'm always curious to know how to improve my own flow by how others have found theirs to have improved.\\n\\n## Conclusion\\n\\nThat's it for Part 3 of this series. Now we know that:\\n\\n- We're in control\\n- Specificity is key\\n- AI is binary and schizophrenic\\n- Time and energy is a precious commodity\\n- Language contracts (keywords) are optimal\\n\\nThe more you interact with Generative AI, the more you'll become comfortable with the process. It can be daunting at first to meet a new being and learn its language, and how to present yourself to it. Never forget, though, that GenAI is merely a reflection of the human species. It's quite easy in some ways, and in others it's quite frustrating. But in both types of way, it's very possible to perform work.\\n\\nRemember, you're always in control.\\n\\nSee you in the next article!	6	engineering	{}	Today we dig into the schizophrenic nature of AI and how to tackle it with attention, attention, attention, language contracts, addressing errors immediately whenever they happen, and relating to your copilot.	t
8	millenials-toys-r-us-death-by-bad-leveraged-buyout	What killed Toys “R” Us? It wasn’t Amazon	Amazon kills. Its sale-prediction technology is the queen, its logistics infrastructure the king. To their competitors, they are not merciful. But, strong as they are, this isnt another death-by-Bezos.	2018-11-06 00:00:00-07	> I don't wanna grow up, I'm a Toys R Us kid. They got a million toys at Toys R Us that I can play with! From bikes to trains to video games, it's the biggest toy store there is! I don't wanna grow up, cause maybe, if I did, I couldn't be a Toys R Us kid...\\n>\\n> *I don't wanna grow up! You wanna grow up??*\\n>\\n> I wanna be a Toys R Us kid!\\n\\nEvery month, we're hearing of another major retailer suffering some form of death by Amazon's hand. Oh, Bezos, you cruel monster! Must you not destroy every symbol of our adolescence! Allow us some preservation, but I still want my Amazon Prime!\\n\\n**Amazon kills.**\\n-----------------\\n\\nIts sale-prediction technology is the queen, and its logistics infrastructure, proudly the king. Sooner or later, every seller of fungible goods will **die** by the Sword of Bezos!\\n\\nRecently, my #millennial generation's beloved [Toys \\"R\\" Us formally announced its liquidation](https://www.wsj.com/articles/toys-r-us-tells-workers-it-will-likely-close-all-u-s-stores-1521060803). **Poof.** As quick as the snap of a finger, my childhood memory felt a pain. The childhood memory becomes an adulthood memorial, almost immediately.\\n\\nHow could Toys \\"R\\" Us have failed us so dramatically?\\n-----------------------------------------------------\\n\\nAbove all, they were the biggest toy store there was! After reaching epic levels of success and brand identity awareness in the adolescent commercial market, what were the odds they would have gone bankrupt?\\n\\n### They had everything.\\n\\nDuring their height, they had it all:\\n\\n-   Power Wheels\\n-   killer bikes\\n-   remote-controlled cars\\n-   battery-powered trains\\n-   awesome action figures\\n-   bouncy balls\\n-   board games\\n-   video games\\n-   playpens\\n-   sports gear\\n-   practically *anything* a kid would want to put on his wish list for Santa!\\n\\nThey *owned* the toy market. In fact, their reason for existence was to be **the** one-stop shop for anything a kid's heart desired. In likely consequence, a parent of young children could literally clear an entire holiday shopping list with a single visit.\\n\\n#### In the end, they had nothing.\\n\\nAs soon its benevolent owners had encumbered its balance sheet, Toys \\"R\\" Us had no shot. When Toys \\"R\\" Us needed liquid capital for a large-scale war against Amazon, they fought a war equally against the phantom weight of debt interest.\\n\\nEventually, their time was up.\\n\\nNow, Toys \\"R\\" Us and its owners have reduced itself merely to a brand identity. Today perhaps it's so fortunate and that its name and logo survive the sale of everything else.\\n\\nI was incredulous.\\n\\n> *Amazon can't be so good.*\\n>\\n> *They can't be this cruel! *\\n>\\n> *Is this **really** another #deathbybezos ?*\\n\\n#### It ***isn't*** actually an Act of the Amazon, but it certainly feels like one.\\n\\nThe very long, tortured demise of Toys \\"R\\" Us began many, many moons ago. To tell this story, we must go way back in time... to the summer of 2005.\\n\\n### The Golden Years for sure, amirite?\\n\\nAhhhhh yeahhh, the year two thousand five. 13 years ago, baby.\\n\\n*Well... I mean, you know... **except***** for** full-scale invasion and occupation of the Middle East, and the full amount of that tab to be borne by our posterity.\\n\\nBut don't forget! We also **were looking on the bright side:**\\n\\n1.  the Dot-Com Crash was behind us\\n2.  the Housing Bubble was reaching all-time highs\\n3.  Credit was a chocolate stream\\n4.  Economy was booming!\\n\\nWhile the internet continued its global crusade of technological progress, more humans used the medium for social communication. More sales were going online.\\n\\nAmazon Prime was only a few months old. But it wasn't a known thing to most shoppers yet, because social media (and two-day shipping) was a very new thing in 2005. The digital sales days were still in infancy. As gorilla-sized, niche-occupying retailers like Toys \\"R\\" Us each proved a fresh retail boutique sales model, they collectively were given a new moniker: They now were known *category killers.* Who among them was to worry? What possibly could retailers awash in cash really have to worry about? They were busy slaughtering their competition's year-end figures --- with ease! They were riding high!\\n\\nThey clearly were nowhere near the entrance to a sad realm of torturous pain and worry and financial dire straits.\\n\\n#### ... or *were* they? ...\\n\\nEnter three companies:\\n\\n-   Bain Capital\\n-   Kohlberg Kravis Roberts\\n-   Vornado Realty Trust\\n\\nWhat do they all have in common?\\n\\nIf you really want to know what's responsible for killing Toys \\"R\\" Us, I can explain it in two words: **Leveraged buyout.**\\n\\nThat Wall Street trio sucked the value completely from Toys \\"R\\" Us's balance sheet, as it took a long, final ride, to the bottom. From the moment they closed their $6.6 billion debt deal, Toys \\"R\\" Us was destined for Chapter 7.\\n\\n*Everything Dies.*	4	other	{}	Amazon kills. Its sale-prediction technology is the queen, its logistics infrastructure the king. To their competitors, they are not merciful. But, strong as they are, this isnt another death-by-Bezos.	t
17	my-confession-of-my-intellectual-inferiority-and-ineptitude	My Confession of My Intellectual Inferiority and Ineptitude	This is what you need to know about me before you consider hiring me. I wouldn't belong in your company unless you agree with me on these points.	2026-01-09 00:00:00-07	This is a confession. If it forever bars me from employment, then so be it. I've endured worse.\\n\\nThe short truth of the matter is that I don't belong here and never have belonged here. I was ignorant of badness throughout my childhood and early adulthood, and I was happy as a result.\\n\\nIf I can summarize all of it in one sentence, it is this:\\n\\n## I'm a good person, and the world is a bad place.\\n\\nIf I could be a bad person, I'd fit right in with everybody. But I'm a good person. That's the crux of my intractible problem in this life. I cannot be like you, because I'm barred from being like you. I was raised to be a good person with intelligence. That combination doesn't seem to exist. And because it doesn't seem to exist, it doesn't exist to me, period.\\n\\nI wish I could be like you all. It must be so easy to do whatever you do on a daily basis and give little care about the consequences of your actions upon good people such as me. It is a tragedy that the world has given itself so much to bad behavior, and that I couldn't become like you soon enough to overcome my deficiencies. But what is done is done. It is what it is. I am what I am, and I cannot change that. Do with me what you will, because I cannot change you nor myself.\\n\\n## If the world wasn't a bad place, I'd be able to do great things for everybody.\\n\\nIf I was allowed to achieve my dreams, I'd be able to do so much good for you all. We all could prosper forever.\\n\\nI'm not going to share my ideas about such a direction in life, because as soon as I share my dreams, you'll take them and run with them in ways I cannot control nor enjoy. That is the world's loss, and it's been my loss too. But that is the nature of life, evidently. It isn't my nature. I don't want to give a bad world anymore firepower than it already has to use against me.\\n\\nSo, I won't try to do good things for everybody, as making such an effort is futile, fruitless, and foolish. I'd rather die than give anymore firepower to the world that has cast me out for many years, hidden the truth from me for even more years, and made a laughingstock out of me for as long as I've been old enough to try to make my own way in life.\\n\\n## Giftedness is a curse upon the person whom is given it.\\n\\nYes, I've been \\"gifted.\\" I was placed in a higher level of schooling for many years. I was given opportunity under the pretense of gainful employment. I was given money, a playground for my creativity, enough support to boost my mood and keep me in the dark, and to explore in ways I never had the ability to do so when I was younger.\\n\\nAnd what happened as a consequence? I was led into defeat to such an extent that I was forced to become aware of my condition in ways a person should never become so aware. The awareness has completely destroyed me on both sides of the equation. I died, figuratively.\\n\\nGiftedness is simply a setup for failure at a later date and in much greater magnitude, if the person so gifted is a good person.\\n\\n## I wish I had never been \\"gifted.\\"\\n\\nI wish I never was gifted, so I never would have become so aware of how great it can be to be a bad person. Good people exist happily so long as they aren't given awareness of the bare truths about badness.\\n\\nI could have gone on in life thinking I was smart, when I was really dumb and stupid, hobbled with disability, and supported by a strong enough ego to overcome enough self-doubt. But no, that wouldn't satisfy the world enough, as it didn't. It had to experiment with me, fuck around with me, play me for a fool, get more data from my behavior, turn me into an exemplifier of ways to defeat good people by being a list of such examples and demonstrating the consequence of such experimentation upon a human being in a natural enough environment.\\n\\nI was experimented on. And boy did the world get its data from me, whether or not I wanted to provide it. It found the ways on how to destroy a good person completely, and I was the conduit. Well done, World.\\n\\nThat's been my purpose in life, unbeknownst to me, to be a conduit of others' experimentation, and nothing more, really. Except I am a human being whom has a family and had a circle of friends, all of whom have loved and/or cared for me throughout my life. They've (perhaps unwittingly) participated in this charade called my life, and a lot of people have learned a lot from it.\\n\\nI guess that's the takeaway. I doubt it's a positive one, but as I already said, the truth is that it is what it is. That's it.\\n\\nThere it is. A massive confession of my desire to relieve myself of the burden of giftedness. Do with it what you will, because I'm tired of pretending and trying to better myself in my condition I have unfortunately become so aware of.\\n\\n## If you don't agree with me on those points, then there's no point in us talking about anything.\\n\\nLet me know if you wish to talk. Hit my contact button and send me a message. Otherwise, I wish you the best of luck, as I don't see much if anything I can do to help you.	5	other	{}	This is what you need to know about me before you consider hiring me. I wouldn't belong in your company unless you agree with me on these points.	f
\.


--
-- Data for Name: tags; Type: TABLE DATA; Schema: public; Owner: alden
--

COPY public.tags (id, name, slug) FROM stdin;
1	Engineering	engineering
2	Cinematic	cinematic
3	Process	process
4	Web Development	web-development
5	Web Design	web-design
6	Video Production	video-production
7	Video Editing	video-editing
\.


--
-- Name: tags_id_seq; Type: SEQUENCE SET; Schema: public; Owner: alden
--

SELECT pg_catalog.setval('public.tags_id_seq', 7, true);


--
-- Name: categories categories_pkey; Type: CONSTRAINT; Schema: public; Owner: alden
--

ALTER TABLE ONLY public.categories
    ADD CONSTRAINT categories_pkey PRIMARY KEY (id);


--
-- Name: posts posts_pkey; Type: CONSTRAINT; Schema: public; Owner: alden
--

ALTER TABLE ONLY public.posts
    ADD CONSTRAINT posts_pkey PRIMARY KEY (id);


--
-- Name: tags tags_pkey; Type: CONSTRAINT; Schema: public; Owner: alden
--

ALTER TABLE ONLY public.tags
    ADD CONSTRAINT tags_pkey PRIMARY KEY (id);


--
-- PostgreSQL database dump complete
--

\unrestrict x3hVOQiGCzzfRW2bkBr2w32XxsLQgevn7W0DTmyPzzg8kvFx5eW8wmX2mcYfFuq

